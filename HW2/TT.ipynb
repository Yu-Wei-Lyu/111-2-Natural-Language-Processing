{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920\n",
      "920 Counter({'O': 128263, 'B-PER': 7869, 'B-ORG': 4013, 'B-LOC': 2678, 'I-PER': 1539, 'I-ORG': 1114, 'I-LOC': 920})\n",
      "{'O': 128263, 'B-PER': 7869, 'B-ORG': 4013, 'B-LOC': 2678, 'I-PER': 1539, 'I-ORG': 1114, 'I-LOC': 920}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision： [0.37368421 0.22340426 0.16352201 0.         0.         0.5\n",
      " 0.95050905]\n",
      "Recall： [0.49305556 0.17213115 0.44067797 0.         0.         0.06666667\n",
      " 0.9491669 ]\n",
      "F1-score： [0.4251497  0.19444444 0.23853211 0.         0.         0.11764706\n",
      " 0.9498375 ]\n",
      "Accuracy： 0.8741522230595328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision： 0.31587421831397666\n",
      "Recall： 0.3030997482671406\n",
      "F1-score： 0.27508725938925777\n",
      "Accuracy： 0.8741522230595328\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import *\n",
    "from sklearn.metrics import *\n",
    "from collections import Counter\n",
    "\n",
    "def get_minimum_class(file_name):\n",
    "    classes = []\n",
    "    for file in file_name:\n",
    "        with open(file, encoding='utf-8') as conll:\n",
    "            for line in conll:\n",
    "                line = line.strip()\n",
    "                if line and len(line.split()) == 2: # 排除空行和參數不完整的行\n",
    "                    word, tag_class = line.split()\n",
    "                    classes.append(tag_class)\n",
    "\n",
    "    # 使用 Counter 统计每个标签出现的次数\n",
    "    label_counts = Counter(classes)\n",
    "    min_count = min(label_counts.values())\n",
    "    # 输出统计结果\n",
    "    print(min_count)\n",
    "    return min_count\n",
    "\n",
    "def read_train_conll_file(file_name):\n",
    "    current_item = []\n",
    "    words = []\n",
    "    classes = []\n",
    "    ccount = {'O':0, 'B-PER':0, 'B-ORG':0,'B-LOC':0,'I-PER':0,'I-ORG':0,'I-LOC':0}\n",
    "    min_tag_amount = get_minimum_class(file_name)\n",
    "    for file in file_name:\n",
    "        with open(file, encoding='utf-8') as conll:\n",
    "            for line in conll:\n",
    "                line = line.strip()\n",
    "                if line and len(line.split()) == 2: # 排除空行和參數不完整的行\n",
    "                    word, tag_class = line.split()\n",
    "                    if True or ccount[tag_class] < min_tag_amount:\n",
    "                        current_item.append((word, tag_class))\n",
    "                        ccount[tag_class] += 1\n",
    "                        words.append(words)\n",
    "                    classes.append(tag_class)\n",
    "\n",
    "    # 使用 Counter 统计每个标签出现的次数\n",
    "    label_counts = Counter(classes)\n",
    "    min_count = min(label_counts.values())\n",
    "    # 输出统计结果\n",
    "    print(min_count, label_counts)\n",
    "    print(ccount)\n",
    "    return current_item\n",
    "\n",
    "def read_test_conll_file(file_name):\n",
    "    current_item = []\n",
    "    with open(file_name, encoding='utf-8') as conll:\n",
    "        for line in conll:\n",
    "            line = line.strip()\n",
    "            if line and len(line.split()) == 2: # 排除空行和參數不完整的行\n",
    "                word, tag_class = line.split()\n",
    "                current_item.append((word, tag_class))\n",
    "    return current_item\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "'''print(\"a\")\n",
    "read_train_conll_file(['a.conll'])\n",
    "print(\"b\")\n",
    "read_train_conll_file(['b.conll'])\n",
    "print(\"e\")\n",
    "read_train_conll_file(['e.conll'])\n",
    "print(\"f\")\n",
    "read_train_conll_file(['f.conll'])\n",
    "print(\"g\")\n",
    "read_train_conll_file(['g.conll'])\n",
    "print(\"h\")\n",
    "read_train_conll_file(['h.conll'])'''\n",
    "train_set=[(word_feats(words), tag_classes) for (words, tag_classes) in read_train_conll_file(['a.conll',  'b.conll', 'f.conll', 'g.conll', 'h.conll'])]\n",
    "test_set=[(word_feats(words), tag_classes) for (words, tag_classes) in read_test_conll_file('e.conll')]\n",
    "\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "test_predict = []\n",
    "testing = []\n",
    "for item in test_set:\n",
    "    test_predict.append(classifier.classify(item[0]))\n",
    "    testing.append(item[1])\n",
    "\n",
    "precision = precision_score(testing, test_predict, average=None)\n",
    "recall = recall_score(testing, test_predict, average=None)\n",
    "f1 = f1_score(testing, test_predict, average=None)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "print(f'Precision： {precision}')\n",
    "print(f'Recall： {recall}')\n",
    "print(f'F1-score： {f1}')\n",
    "print(f'Accuracy： {accuracy}')\n",
    "\n",
    "precision = precision_score(testing, test_predict, average='macro')\n",
    "recall = recall_score(testing, test_predict, average='macro')\n",
    "f1 = f1_score(testing, test_predict, average='macro')\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "print(f'Precision： {precision}')\n",
    "print(f'Recall： {recall}')\n",
    "print(f'F1-score： {f1}')\n",
    "print(f'Accuracy： {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
